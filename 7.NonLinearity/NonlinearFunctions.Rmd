Nonlinear Models
==============================================================================================
```{r}
require(ISLR)
attach(Wage)
```

Polynomials
------------
```{r}
fit=lm(wage~poly(age,4),data=Wage)
summary(fit)
```

The 'poly()' function generates a basis of *orthogonal polynomials*.
Lets make a plot of the fitted function, along with the standard errors of the fit.

```{r fig.width=7, fig.height=6}
agelims = range(age)
age.grid = seq(from=agelims[1], to = agelims[2])
preds=predict(fit, newdata=list(age=age.grid), se=TRUE)
se.bands=cbind(preds$fit+2*preds$se, preds$fit-2*preds$se)
plot(age, wage, col='darkgrey')
lines(age.grid, preds$fit, lwd=2, col='blue')
matlines(age.grid, se.bands, col='blue', lty=2)
```

Another way to do so is to fit the polynimial directly. 
```{r}
# We use identity function to square, otherwise it has anohter meaning
fita = lm(wage~age+I(age^2) + I(age^3) + I(age^4), data = Wage)
summary(fita)
```

The coeff and the p-value are different because we are using different basis for
representing the polyomial. The fitted polynomial was the same, but the 
representation was different. 

```{r}
plot(fitted(fit), fitted(fita))
```

By using the orthogonal polynomials, we can test each of the coefficients separately and decide which ones are not needed. This idea only works with linear regression, and if there is a single predictor. In general, to test
wheter a one degree polynomial is better than another degree, you need to 
use ANOVA function. (e.g. if we have other variables)

```{r}
# Nested sequence of models: previous models contained as special cases
fita=lm(wage~education, data=Wage)
fitb=lm(wage~education+age, data=Wage)
fitc=lm(wage~education+poly(age,2), data=Wage)
fitd=lm(wage~education+poly(age,3), data=Wage)
anova(fita, fitb, fitc, fitd)
```

The anova summary tell us that age is needed in the model with education. 
Also age squared is certainly needed as well as age in the model. But age
cubed is not necessarily needed, it is not quite significant.

### Polynimial logistic regression

Now we fit a logistic regression model to a binary response variable, constructed from 'wage'. We code the big earners ('>250K') as 1, else 0.

```{r}
fit = glm(I(wage>250) ~ poly(age,3), data=Wage, family=binomial)
summary(fit)
```

Because we use GLM, even though the polynimial is an orthogonal basis, it's
no longer strictly orthogonal. GLM involves having weights for the observations,
and so the orthogonality is somewhat lost. If we really want to do the test,
if a polynimial of degree 3 was needed, we'd have to fit another GLM wiht a polyomial of degree 2.

```{r}
preds = predict(fit, list(age=age.grid), se=TRUE)
# We want preds$fit to be in the middle of the standard error bands. 
# 2 std use to calculate an estimated confidence interval, a 95% confidence
se.bands = preds$fit + cbind(fit=0, lower=-2*preds$se, upper=2*preds$se)
se.bands[1:5,]
```

We have done the computations on the logit scale. To transform in probability
scale, we need to apply the inverse logit mapping:
$$p=\frac{e^\eta}{1+e^\eta}$$

```{r}
# The confidence band's (standard error bands for the pobabilities) all lie
# within 0 and 1.

prob.bands=exp(se.bands)/(1+exp(se.bands))
matplot(age.grid, prob.bands, col='blue', lwd=c(2,1,1), lty = c(1,2,2),
        type='l', ylim=c(0,.1))

# jitter of age, because there were lots of ties in age at each of the integer values of age. To give an indication of how much data occurred, I used the 
# Jitter which randomly adds to each element of the vector age, a little bit of uniform random noise, so we can get an idea of the density. 
points(jitter(age), I(wage>250)/10, pch='I', cex=.5)

```

There are only 4%, at most, of the population in any given age category earned
above 250K.

SPLINES
--------
1. FIXED  KNOT REGRESSION SPLINES

Splines are more flexible than polynimials, but the idea is ratehr similar. 
```{r}
require(splines)
# fit a cubic spline with knots at 25,40 and 60
fit = lm(wage~bs(age, knots = c(25,40,60)), data = Wage)
plot(age, wage, col='darkgrey')
lines(age.grid, predict(fit, list(age=age.grid)), col='darkgreen', lwd=2)
abline(v=c(25,40,60), lty=2, col='darkgreen')
```

A spline is a cubic polynimial at defined knots (25, 40, 60), which are places 
of discontinuity. They're just disontinuity in the third derivative. 
They are cubic polynomials in each regions, but they contrain to be continuous at first and second derivatives which make them very smooth. 

They are more local than polynomials (not wagging tails).

2. SMOOTHING SPLINES does not require knot selection, but it does have a 
smoothing parameter, which can conveniently be specified via the effective 
degrees of freedom or 'df'. Essentially, they have knots everywhere, and they control how smooth a function is with the roughness penalty. 

```{r}
fit = lm(wage~bs(age, knots = c(25,40,60)), data = Wage)
plot(age, wage, col='darkgrey')
lines(age.grid, predict(fit, list(age=age.grid)), col='darkgreen', lwd=2)
abline(v=c(25,40,60), lty=2, col='darkgreen')
fit2 = smooth.spline(age, wage, df=16)
lines(fit2, col='red', lwd=2)
# Line is wiggling due to the high degree of freedom in this case. 
```


Alternatively, we can use Leave One Out cross-validation to select the
smoothing parameter for us automatically:
```{r}
fit = lm(wage~bs(age, knots = c(25,40,60)), data = Wage)
plot(age, wage, col='darkgrey')
lines(age.grid, predict(fit, list(age=age.grid)), col='darkgreen', lwd=2)
abline(v=c(25,40,60), lty=2, col='darkgreen')
fit2 = smooth.spline(age, wage, df=16)
lines(fit2, col='red', lwd=2)
fit3 = smooth.spline(age, wage, cv=TRUE)
lines(fit3, col='purple', lwd=2)
# less effective degrees of freedom, an heuristic for how rough the function is. In this case, about siz to seven degrees of freedom.
fit3
```


GENERALIZED ADDITIVE MODELS
----------------------------

So far we have focused on fitting models with mostly single nonlinear terms.
The `gam` package makes it easier to work with multiple nonlinear terms. In
addition it knows how to plot these functions and their standard errors. 

```{r}
require(gam)
gam1 = gam(wage~s(age, df=4)+s(year, df=4)+education, data=Wage)
par(mfrow=c(1,3))
plot(gam1, se=TRUE)

# GAM  also works for logistic regression
gam2 = gam(I(wage>250)~s(age, df=4)+s(year, df=4)+education, data=Wage, family=binomial)
plot(gam2)
```

Lets see if we need a nonlinear terms for year. 

```{r}
# GAM  also works for logistic regression
gam2a = gam(I(wage>250)~s(age, df=4)+year+education, data=Wage, family=binomial)
anova(gam2a, gam2, test='Chisq')
```

The p value is 0.82, which says we really don't need this non-linear term for year. A linear term will be fine. We can test further and see if you need a term at all for year. 

One nice feature of the `gam` package is that it knows how to plot the funcitons nicely, even for models fit by `lm` and `glm`.

```{r fig.width=10, fig.height=5}
par(mfrow = c(1,3))
lm1 = lm(wage~ns(age, df=4)+ns(year, df=4) + education, data=Wage)
plot.Gam(lm1, se= TRUE)
```

