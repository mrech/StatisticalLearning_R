Model Selection
====================

```{r}
library(ISLR)
summary(Hitters)
```

Remove the missing values:
```{r}
Hitters=na.omit(Hitters)
with(Hitters, sum(is.na(Salary)))
```

Best Subset regression
------------------------
With the package 'leaps' evaluate all the best-subset models.

```{r}
library(leaps)
regfit.full=regsubsets(Salary~.,data=Hitters)
summary(regfit.full)
```
It gives by default best-subsets up to size 8; let's include all the x variables 
```{r}
regfit.full=regsubsets(Salary~.,data=Hitters, nvmax = 19)
reg.summary=summary(regfit.full)
names(reg.summary)
plot(reg.summary$cp, xlab = 'Number of Variables', ylab = 'Cp')
which.min(reg.summary$cp)
points(10,reg.summary$cp[10],pch=20,col='red')
```
Alternative plot
```{r}
plot(regfit.full, scale = 'Cp')
coef(regfit.full,10)
```

Forward Stepwise Selection
--------------------------
```{r}
regfit.fwd = regsubsets(Salary~.,data=Hitters, nvmax=19, method='forward')
summary(regfit.fwd)
plot(regfit.fwd, scale='Cp')
```

Model Selection Using a Validation Set
--------------------------------------
```{r}
dim(Hitters)
set.seed(1)
# we select 2/3 for the training set
train = sample(seq(263),180,replace = FALSE)
regfit.fwd = regsubsets(Salary~.,data=Hitters[train,], nvmax = 19, method = 'forward')
```

Now we make predictions on the observations not used for training. 
There are 19 models, so we set up some vectors to record the errors.
```{r}
val.errors=rep(NA, 19)
x.test=model.matrix(Salary~.,data=Hitters[-train,])
for(i in 1:19){
  coefi=coef(regfit.fwd,id=i)
  pred=x.test[,names(coefi)]%*%coefi # matrix multiplication
  val.errors[i]=mean((Hitters$Salary[-train]-pred)^2)
}
plot(sqrt(val.errors), ylab='Root MSE', ylim=c(300,400),pch=19,type = 'b')
# The residual sum of squares divided by the number of degrees of freedom
points(sqrt(regfit.fwd$rss[-1]/180), col='blue', pch=19,type='b')
legend('topright', legend=c('Training','Validation'), col=c('blue','black'),pch = 19)

```
As we expect, the training error goes down monotonically as the model gets bigger, 
but not so for the validation error. 

Create a function for making predictions with 'regsubsets'.
```{r}
predict.regsubsets=function(object, newdata, id, ...){
  form=as.formula(object$call[[2]])
  mat=model.matrix(form, newdata)
  coefi = coef(object, id)
  mat[,names(coefi)]%*%coefi
}
```

Model Selection by Cross-Validation
-----------------------------------

Using 10-fold Cross-Validation
```{r}
set.seed(11)
folds=sample(rep(1:10,length = nrow(Hitters)))
# Check if we have balanced observations
table(folds)
# 10 rows for each folds and 19 columns for each subset of the predictors
cv.errors = matrix(NA,10,19)
for(k in 1:10){
  best.fit=regsubsets(Salary~.,data = Hitters[folds!=k,],nvmax=19,method = 'forward')
  for (i in 1:19){
    pred=predict(best.fit,Hitters[folds==k,],id=i)
    cv.errors[k,i]=mean((Hitters$Salary[folds==k]-pred)^2)
  }
}
# average down the columns (mean errors for each folds)
rmse.cv=sqrt(apply(cv.errors,2,mean))
plot(rmse.cv,pch=19,type='b')
```

Ridge Regression and the Lasso
------------------------------
```{r}
library(glmnet)
x=model.matrix(Salary~.-1,data=Hitters)
y=Hitters$Salary
```

Fit a ridge-regression model.
This is achieved by calling 'glmnet' with 'alpha=0'.
Using 'cv.glmnet' will do the cross-validation for us.
```{r}
fit.ridge = glmnet(x,y,alpha = 0)
plot(fit.ridge,xvar='lambda',label=TRUE)
cv.ridge = cv.glmnet(x,y,alpha=0)
plot(cv.ridge)
```

Now we fit a lasso model.
For this we use the default 'alpha=1'.
```{r}
fit.lasso=glmnet(x,y)
plot(fit.lasso, xvar='lambda', label=TRUE)
cv.lasso=cv.glmnet(x,y)
plot(cv.lasso)
coef(cv.lasso)
```

train/validation division to select the 'lambda' for the lasso. 
```{r}
lasso.tr=glmnet(x[train,],y[train])
lasso.tr
pred = predict(lasso.tr,x[-train,])
dim(pred)
rmse=sqrt(apply((y[-train]-pred)^2, 2, mean))
plot(log(lasso.tr$lambda),rmse,type='b',xlab = 'Log(lambda)')
lam.best=lasso.tr$lambda[order(rmse)[1]]
lam.best
coef(lasso.tr,s=lam.best)
```

